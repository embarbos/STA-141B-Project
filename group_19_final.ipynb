{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 19 Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Acquisition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import requests\n",
    "import twint\n",
    "import nest_asyncio\n",
    "import json\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = twint.Config()\n",
    "c.Search = 'Nuclear Energy'\n",
    "c.Limit = 5000\n",
    "c.Store_json = True\n",
    "#c.Output = 'twit_data.json'\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = twint.Config()\n",
    "c.Search = 'Nuclear Power'\n",
    "c.Limit = 5000\n",
    "c.Store_json = True\n",
    "#c.Output = 'twit_data.json'\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r'twit_data.json', 'rb')\n",
    "data = [json.loads(line) for line in f]\n",
    "tweet_df = pd.DataFrame(data)\n",
    "tweet_df = tweet_df[['tweet','link']]\n",
    "tweet_df = tweet_df.drop_duplicates()\n",
    "dataremoved = data[~data.iloc[:,0].str.contains('Ukraine')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('ukraine')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('Ukrainian')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('ukrainian')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('Ukrainians')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('ukrainians')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('Zaporizhzhia')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('war')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('russia')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('Russia')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('russian')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('Russian')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('russians')]\n",
    "dataremoved = dataremoved[~data.iloc[:,0].str.contains('Russians')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "words = []\n",
    "for tweet in dataremoved['tweet']:\n",
    "    clean = re.sub(r\"(http[s]?\\://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \"\", tweet)\n",
    "    clean = re.sub(r\"\\d\", '', clean)\n",
    "    clean = re.sub(r\"'\\S+\", '', clean)\n",
    "    clean = clean.replace('.', '').replace(';', '').lower()\n",
    "    words += re.findall(r\"(?:\\w+|'|â€™)+\", clean)\n",
    "    cleaned_tweets.append(clean)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "standardized = [w for w in words if w not in stopwords]\n",
    "\n",
    "def wordnet_pos(tag):\n",
    "    \"\"\"Map a Brown POS tag to a WordNet POS tag.\"\"\"\n",
    "\n",
    "    table = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
    "\n",
    "    # Default to a noun.\n",
    "    # if the key doesn't exist, it will return a wordnet.NOUN\n",
    "    return table.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "# all pos_tags for lower words\n",
    "twit_tags = nltk.pos_tag(standardized)\n",
    "\n",
    "# obtain tags\n",
    "tags = [wordnet_pos(x[1]) for x in twit_tags]\n",
    "\n",
    "# create a new sentence\n",
    "new_text = \" \".join(w for w in standardized)\n",
    "blob = TextBlob(new_text)\n",
    "# obtain tags\n",
    "tags = [wordnet_pos(x[1]) for x in blob.pos_tags]\n",
    "\n",
    "# finalize the lemmatization\n",
    "new_text = \" \".join(x.lemmatize(t) for x, t in zip(blob.words, tags))\n",
    "# words after lemmatization\n",
    "standardized_words = TextBlob(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Model: Preliminary LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Model: K-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third Model: VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closing Statements:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2afbdf73fbcdecb6073a7d3fe1b85cb5ab8042f504d040d34130e4f01d9f1260"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
