{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 19 Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweet_df Acquisition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tuomasr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tuomasr/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tuomasr/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/tuomasr/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import requests\n",
    "import twint\n",
    "import nest_asyncio\n",
    "import json\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = twint.Config()\n",
    "# c.Search = 'Nuclear Energy'\n",
    "# c.Limit = 5000\n",
    "# c.Store_json = True\n",
    "# c.Output = 'twit_data.json'\n",
    "# twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = twint.Config()\n",
    "# c.Search = 'Nuclear Power'\n",
    "# c.Limit = 5000\n",
    "# c.Store_json = True\n",
    "# c.Output = 'twit_data.json'\n",
    "# twint.run.Search(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataremoved Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r'twit_data.json', 'rb')\n",
    "data = [json.loads(line) for line in f]\n",
    "tweet_df = pd.DataFrame(data)\n",
    "tweet_df = tweet_df[['tweet','link']]\n",
    "tweet_df = tweet_df.drop_duplicates()\n",
    "dataremoved = tweet_df[~tweet_df.iloc[:,0].str.contains('Ukraine')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('ukraine')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Ukrainian')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('ukrainian')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Ukrainians')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('ukrainians')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Zaporizhzhia')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('war')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('russia')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Russia')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('russian')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Russian')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('russians')]\n",
    "dataremoved = dataremoved[~dataremoved.iloc[:,0].str.contains('Russians')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "words = []\n",
    "for tweet in dataremoved['tweet']:\n",
    "    clean = re.sub(r\"(http[s]?\\://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \"\", tweet)\n",
    "    clean = re.sub(r\"\\d\", '', clean)\n",
    "    clean = re.sub(r\"'\\S+\", '', clean)\n",
    "    clean = clean.replace('.', '').replace(';', '').lower()\n",
    "    words += re.findall(r\"(?:\\w+|'|’)+\", clean)\n",
    "    cleaned_tweets.append(clean)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "standardized = [w for w in words if w not in stopwords]\n",
    "\n",
    "# removing other symbols\n",
    "corpus = [[re.sub('[^a-zA-Z ]', ' ', document)] for document in cleaned_tweets]\n",
    "#tokenizing\n",
    "corpus_tokenized = [nltk.word_tokenize(document[0]) for document in corpus]\n",
    "# stop words\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "corpus_tokenized = [[word for word in document if word not in stopwords] for document in corpus_tokenized]\n",
    "#lemmatizing\n",
    "nltk.download('wordnet')\n",
    "corpus_lemmatized = [[nltk.WordNetLemmatizer().lemmatize(word) for word in document] for document in corpus_tokenized]\n",
    "#stitching back together\n",
    "corpus = [' '.join(document) for document in corpus_lemmatized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(another cleaning method-- on a per tweet basis. we need to sort this out w/ the code above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#all.csv is in the github, it is all of the tweets including the training set tweets we worked on, with the ukraine/russian/war tweets filtered out\n",
    "data = pd.read_csv('all.csv',header=None)\n",
    "#%%\n",
    "data = data.iloc[:,1]\n",
    "data.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "words = []\n",
    "for tweet in data:\n",
    "    clean = re.sub(r\"(http[s]?\\://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \" \", tweet)\n",
    "    clean = re.sub(r\"\\d\", '', clean)\n",
    "    clean = re.sub(r\"'\\S+\", '', clean)\n",
    "    clean = clean.replace('.', '').replace(';', '').lower()\n",
    "    words += re.findall(r\"(?:\\w+|'|’)+\", clean)\n",
    "    cleaned_tweets.append(clean)\n",
    "    \n",
    "    \n",
    "# removing other symbols\n",
    "corpus = [[re.sub('[^a-zA-Z ]', ' ', document)] for document in cleaned_tweets]\n",
    "#tokenizing\n",
    "corpus_tokenized = [nltk.word_tokenize(document[0]) for document in corpus]\n",
    "# stop words\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "corpus_tokenized = [[word for word in document if word not in stopwords] for document in corpus_tokenized]\n",
    "#lemmatizing\n",
    "corpus_lemmatized = [[nltk.WordNetLemmatizer().lemmatize(word) for word in document] for document in corpus_tokenized]\n",
    "#stitching back together\n",
    "corpus_string = [' '.join(document) for document in corpus_lemmatized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model: Preliminary LDA\n",
    "\n",
    "We will be using the gensim package's LDA model because it seems to have more LDA-specific features such as coherence score calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "#import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(corpus_lemmatized)\n",
    "doc_term_matrix = [dictionary.doc2bow(rev) for rev in corpus_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=2, random_state=100,\n",
    "                chunksize=1000, passes=50,iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic display\n",
    "posterior = lda_model.print_topics()\n",
    "two_topic_LDA = pd.DataFrame(posterior)[1]\n",
    "two_topic_LDA = two_topic_LDA.transpose()\n",
    "two_topic_LDA.index = ['topic ' + str(i) for i in range(0,2)]\n",
    "two_topic_LDA.name = 'words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity and coherence scores\n",
    "\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(doc_term_matrix,total_docs=10000))  # a measure of how good the model is. lower the better.\n",
    "# Compute Coherence Score\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus_lemmatized, dictionary=dictionary , coherence='u_mass')\n",
    "if __name__ == \"__main__\":\n",
    "    #freeze_support()\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to identify what number of LDA topics would work best as an input to k-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% GRAPH FUNCTION\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LDA(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build list of scores across different topic numbers\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=doc_term_matrix, texts=corpus_lemmatized, start=2, limit=50, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=50; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal number = 2??? hahahaha\n",
    "\n",
    "INTERACTIVE ELEMENT CAN GO HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MATRIX TIME, THIS OUTPUT WILL GO INTO K-NN\n",
    "\n",
    "tweet_vectors = pd.Series(0)\n",
    "for i in range(len(doc_term_matrix)):    \n",
    "    tweet_vectors[i] = lda_model.get_document_topics(doc_term_matrix[i], minimum_probability=0, minimum_phi_value=None, per_word_topics=False)\n",
    "\n",
    "tweet_vectors_entries = [[tweet_vectors[i][0][1],tweet_vectors[i][1][1]] for i in range(len(tweet_vectors))]\n",
    "\n",
    "LDA_tweet_frame = pd.DataFrame(tweet_vectors_entries, columns = ['Topic 0','Topic 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model: K-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Model: VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = word_tokenize(corpus, \"english\")\n",
    "score = SentimentIntensityAnalyzer().polarity_scores(corpus)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2afbdf73fbcdecb6073a7d3fe1b85cb5ab8042f504d040d34130e4f01d9f1260"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
